# í™”ì¬ ìœ„í—˜ ì˜ˆì¸¡ AI ëª¨ë¸ - ì ì§„ì  í•™ìŠµ ë° Self-Upgrade ì „ëµ

## ğŸ”¥ ë¬¸ì œ ìƒí™©

- **ëª©í‘œ**: í™”ì¬ ìœ„í—˜ ì˜ˆì¸¡ AI ëª¨ë¸ êµ¬ì¶•
- **ì œì•½**: í•™ìŠµ ë°ì´í„° ë¶€ì¡±
- **ìš”êµ¬ì‚¬í•­**: ì ì§„ì  í•™ìŠµ + ìê°€ ê°œì„ (Self-Upgrade)

---

## ğŸ¯ ì¢…í•© ì†”ë£¨ì…˜ ì•„í‚¤í…ì²˜

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ì´ˆê¸° ë‹¨ê³„ (Cold Start)                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. Transfer Learning (ì‚¬ì „í•™ìŠµ ëª¨ë¸)                          â”‚
â”‚ 2. Data Augmentation (ë°ì´í„° ì¦ê°•)                           â”‚
â”‚ 3. Synthetic Data Generation (í•©ì„± ë°ì´í„°)                   â”‚
â”‚ 4. Few-Shot Learning                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 ì ì§„ì  í•™ìŠµ ë‹¨ê³„ (Online Learning)            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. Incremental Learning (ì¦ë¶„ í•™ìŠµ)                          â”‚
â”‚ 2. Active Learning (ëŠ¥ë™ í•™ìŠµ)                               â”‚
â”‚ 3. Semi-Supervised Learning (ì¤€ì§€ë„ í•™ìŠµ)                    â”‚
â”‚ 4. Continual Learning (ì§€ì† í•™ìŠµ)                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Self-Upgrade ë‹¨ê³„ (ìê°€ ê°œì„ )                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. Model Versioning & A/B Testing                           â”‚
â”‚ 2. Feedback Loop (í”¼ë“œë°± ë£¨í”„)                               â”‚
â”‚ 3. AutoML & Hyperparameter Tuning                           â”‚
â”‚ 4. Ensemble Learning (ì•™ìƒë¸”)                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š Phase 1: ì´ˆê¸° ë‹¨ê³„ - ë°ì´í„° ë¶€ì¡± í•´ê²°

### 1.1 Transfer Learning (ì „ì´ í•™ìŠµ) â­â­â­â­â­

**ê°œë…**: ìœ ì‚¬í•œ ë„ë©”ì¸ì—ì„œ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ í™œìš©

**í™”ì¬ ì˜ˆì¸¡ ì ìš©**:
```python
# ì˜ˆì‹œ: ì´ë¯¸ì§€ ê¸°ë°˜ í™”ì¬ ê°ì§€
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D
from tensorflow.keras.models import Model

# 1. ImageNetì—ì„œ ì‚¬ì „í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ
base_model = ResNet50(weights='imagenet', include_top=False, 
                     input_shape=(224, 224, 3))

# 2. ìƒìœ„ ë ˆì´ì–´ë§Œ í•™ìŠµ ê°€ëŠ¥í•˜ê²Œ ì„¤ì •
for layer in base_model.layers[:-10]:
    layer.trainable = False

# 3. í™”ì¬ ì˜ˆì¸¡ìš© ì»¤ìŠ¤í…€ ë ˆì´ì–´ ì¶”ê°€
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(256, activation='relu')(x)
x = Dropout(0.5)(x)
predictions = Dense(3, activation='softmax')(x)  # [ì•ˆì „, ì£¼ì˜, ìœ„í—˜]

model = Model(inputs=base_model.input, outputs=predictions)
```

**í™œìš© ê°€ëŠ¥í•œ ì‚¬ì „í•™ìŠµ ëª¨ë¸**:
- **ì´ë¯¸ì§€**: ResNet, EfficientNet, MobileNet
- **ì‹œê³„ì—´**: Transformer, LSTM ì‚¬ì „í•™ìŠµ ëª¨ë¸
- **ì„¼ì„œ ë°ì´í„°**: AutoEncoder ì‚¬ì „í•™ìŠµ

**ì¥ì **:
- âœ… ì ì€ ë°ì´í„°ë¡œë„ ë†’ì€ ì •í™•ë„
- âœ… ë¹ ë¥¸ í•™ìŠµ ì†ë„
- âœ… ê²€ì¦ëœ íŠ¹ì§• ì¶”ì¶œê¸° í™œìš©

---

### 1.2 Data Augmentation (ë°ì´í„° ì¦ê°•) â­â­â­â­â­

**ê°œë…**: ê¸°ì¡´ ë°ì´í„°ë¥¼ ë³€í˜•í•˜ì—¬ ë°ì´í„°ì…‹ í™•ì¥

#### ì´ë¯¸ì§€ ë°ì´í„° ì¦ê°•
```python
from tensorflow.keras.preprocessing.image import ImageDataGenerator

datagen = ImageDataGenerator(
    rotation_range=20,           # íšŒì „
    width_shift_range=0.2,       # ìˆ˜í‰ ì´ë™
    height_shift_range=0.2,      # ìˆ˜ì§ ì´ë™
    shear_range=0.2,             # ì „ë‹¨ ë³€í™˜
    zoom_range=0.2,              # í™•ëŒ€/ì¶•ì†Œ
    horizontal_flip=True,        # ì¢Œìš° ë°˜ì „
    brightness_range=[0.8, 1.2], # ë°ê¸° ì¡°ì ˆ
    fill_mode='nearest'
)

# ì¦ê°•ëœ ì´ë¯¸ì§€ ìƒì„±
augmented_images = datagen.flow(x_train, y_train, batch_size=32)
```

#### ì„¼ì„œ/ì‹œê³„ì—´ ë°ì´í„° ì¦ê°•
```python
import numpy as np

def augment_sensor_data(data, noise_level=0.01):
    """ì„¼ì„œ ë°ì´í„° ì¦ê°•"""
    augmented = []
    
    # 1. ë…¸ì´ì¦ˆ ì¶”ê°€
    noisy = data + np.random.normal(0, noise_level, data.shape)
    augmented.append(noisy)
    
    # 2. ì‹œê°„ ì™œê³¡ (Time Warping)
    warped = time_warp(data, sigma=0.2)
    augmented.append(warped)
    
    # 3. í¬ê¸° ì¡°ì • (Magnitude Warping)
    scaled = data * np.random.uniform(0.9, 1.1)
    augmented.append(scaled)
    
    # 4. ìœˆë„ìš° ìŠ¬ë¼ì´ì‹±
    window_sliced = window_slice(data, reduce_ratio=0.9)
    augmented.append(window_sliced)
    
    return np.array(augmented)

def time_warp(data, sigma=0.2):
    """ì‹œê°„ì¶• ì™œê³¡"""
    from scipy.interpolate import CubicSpline
    orig_steps = np.arange(data.shape[0])
    
    # ëœë¤ ì™œê³¡ ìƒì„±
    random_warps = np.random.normal(loc=1.0, scale=sigma, 
                                   size=(data.shape[0],))
    warp_steps = np.cumsum(random_warps)
    warp_steps = (warp_steps - warp_steps[0]) / (warp_steps[-1] - warp_steps[0])
    warp_steps = warp_steps * (data.shape[0] - 1)
    
    # ë³´ê°„
    warped = CubicSpline(orig_steps, data)(warp_steps)
    return warped
```

**í™”ì¬ ì˜ˆì¸¡ ë§ì¶¤ ì¦ê°•**:
```python
def fire_risk_augmentation(sensor_data, labels):
    """í™”ì¬ ìœ„í—˜ ì˜ˆì¸¡ íŠ¹í™” ì¦ê°•"""
    augmented_data = []
    augmented_labels = []
    
    for data, label in zip(sensor_data, labels):
        # ì›ë³¸ ë°ì´í„°
        augmented_data.append(data)
        augmented_labels.append(label)
        
        if label == 2:  # ìœ„í—˜ í´ë˜ìŠ¤ (ë¶ˆê· í˜• í•´ì†Œ)
            # ìœ„í—˜ ë°ì´í„°ëŠ” 5ë°° ì¦ê°•
            for _ in range(5):
                # ì˜¨ë„ ë³€ë™ ì‹œë®¬ë ˆì´ì…˜
                temp_variation = data.copy()
                temp_variation[:, 0] += np.random.uniform(-2, 2)  # Â±2ë„
                augmented_data.append(temp_variation)
                augmented_labels.append(label)
                
                # ì—°ê¸° ë†ë„ ë³€ë™
                smoke_variation = data.copy()
                smoke_variation[:, 1] *= np.random.uniform(0.9, 1.1)
                augmented_data.append(smoke_variation)
                augmented_labels.append(label)
    
    return np.array(augmented_data), np.array(augmented_labels)
```

---

### 1.3 Synthetic Data Generation (í•©ì„± ë°ì´í„°) â­â­â­â­

**ê°œë…**: ì‹¤ì œ ë°ì´í„° íŠ¹ì„±ì„ í•™ìŠµí•˜ì—¬ ìƒˆë¡œìš´ ë°ì´í„° ìƒì„±

#### GANì„ ì´ìš©í•œ í•©ì„± ë°ì´í„°
```python
import tensorflow as tf
from tensorflow.keras import layers

def build_generator(latent_dim, output_shape):
    """ìƒì„±ì ëª¨ë¸"""
    model = tf.keras.Sequential([
        layers.Dense(256, activation='relu', input_dim=latent_dim),
        layers.BatchNormalization(),
        layers.Dense(512, activation='relu'),
        layers.BatchNormalization(),
        layers.Dense(np.prod(output_shape), activation='tanh'),
        layers.Reshape(output_shape)
    ])
    return model

def build_discriminator(input_shape):
    """íŒë³„ì ëª¨ë¸"""
    model = tf.keras.Sequential([
        layers.Flatten(input_shape=input_shape),
        layers.Dense(512, activation='relu'),
        layers.Dropout(0.3),
        layers.Dense(256, activation='relu'),
        layers.Dropout(0.3),
        layers.Dense(1, activation='sigmoid')
    ])
    return model

# GAN í•™ìŠµ
latent_dim = 100
generator = build_generator(latent_dim, (100, 10))  # 100 timesteps, 10 features
discriminator = build_discriminator((100, 10))

# ... GAN í•™ìŠµ ë¡œì§ ...

# í•©ì„± ë°ì´í„° ìƒì„±
noise = tf.random.normal([1000, latent_dim])
synthetic_data = generator(noise, training=False)
```

#### VAEë¥¼ ì´ìš©í•œ í•©ì„± ë°ì´í„°
```python
class VAE(tf.keras.Model):
    def __init__(self, latent_dim):
        super(VAE, self).__init__()
        self.latent_dim = latent_dim
        
        # Encoder
        self.encoder = tf.keras.Sequential([
            layers.Input(shape=(100, 10)),
            layers.Flatten(),
            layers.Dense(512, activation='relu'),
            layers.Dense(256, activation='relu'),
        ])
        
        self.z_mean = layers.Dense(latent_dim)
        self.z_log_var = layers.Dense(latent_dim)
        
        # Decoder
        self.decoder = tf.keras.Sequential([
            layers.Input(shape=(latent_dim,)),
            layers.Dense(256, activation='relu'),
            layers.Dense(512, activation='relu'),
            layers.Dense(1000, activation='sigmoid'),
            layers.Reshape((100, 10))
        ])
    
    def encode(self, x):
        h = self.encoder(x)
        return self.z_mean(h), self.z_log_var(h)
    
    def decode(self, z):
        return self.decoder(z)
    
    def reparameterize(self, mean, logvar):
        eps = tf.random.normal(shape=mean.shape)
        return eps * tf.exp(logvar * 0.5) + mean

# VAEë¡œ í•©ì„± ë°ì´í„° ìƒì„±
vae = VAE(latent_dim=20)
# ... í•™ìŠµ í›„ ...
z_sample = tf.random.normal([1000, 20])
synthetic_data = vae.decode(z_sample)
```

#### ë¬¼ë¦¬ ê¸°ë°˜ ì‹œë®¬ë ˆì´ì…˜
```python
def simulate_fire_scenario(initial_temp, duration, fire_probability):
    """í™”ì¬ ì‹œë‚˜ë¦¬ì˜¤ ì‹œë®¬ë ˆì´ì…˜"""
    timesteps = duration * 10  # 0.1ì´ˆ ê°„ê²©
    
    temp = np.zeros(timesteps)
    smoke = np.zeros(timesteps)
    co2 = np.zeros(timesteps)
    
    temp[0] = initial_temp
    
    # í™”ì¬ ë°œìƒ ì‹œë®¬ë ˆì´ì…˜
    fire_start = np.random.randint(timesteps // 2) if np.random.rand() < fire_probability else -1
    
    for t in range(1, timesteps):
        if t > fire_start and fire_start > 0:
            # í™”ì¬ ë°œìƒ í›„
            temp[t] = temp[t-1] + np.random.uniform(2, 5)  # ê¸‰ê²©í•œ ì˜¨ë„ ìƒìŠ¹
            smoke[t] = smoke[t-1] + np.random.uniform(5, 10)
            co2[t] = co2[t-1] + np.random.uniform(50, 100)
        else:
            # ì •ìƒ ìƒíƒœ
            temp[t] = temp[t-1] + np.random.uniform(-0.5, 0.5)
            smoke[t] = max(0, smoke[t-1] + np.random.uniform(-1, 1))
            co2[t] = 400 + np.random.uniform(-10, 10)
    
    return np.stack([temp, smoke, co2], axis=1)

# ë‹¤ì–‘í•œ ì‹œë‚˜ë¦¬ì˜¤ ìƒì„±
synthetic_data = []
synthetic_labels = []

for _ in range(1000):
    # ì•ˆì „ ì‹œë‚˜ë¦¬ì˜¤
    safe_data = simulate_fire_scenario(initial_temp=20, duration=60, fire_probability=0.0)
    synthetic_data.append(safe_data)
    synthetic_labels.append(0)  # ì•ˆì „
    
    # ìœ„í—˜ ì‹œë‚˜ë¦¬ì˜¤
    danger_data = simulate_fire_scenario(initial_temp=25, duration=60, fire_probability=1.0)
    synthetic_data.append(danger_data)
    synthetic_labels.append(2)  # ìœ„í—˜
```

---

### 1.4 Few-Shot Learning (ì†Œìˆ˜ìƒ· í•™ìŠµ) â­â­â­â­

**ê°œë…**: ë§¤ìš° ì ì€ ìƒ˜í”Œë¡œ ìƒˆë¡œìš´ í´ë˜ìŠ¤ í•™ìŠµ

```python
import tensorflow as tf

class PrototypicalNetwork(tf.keras.Model):
    def __init__(self, embedding_dim=64):
        super(PrototypicalNetwork, self).__init__()
        
        self.encoder = tf.keras.Sequential([
            layers.Conv1D(32, 3, activation='relu'),
            layers.MaxPooling1D(2),
            layers.Conv1D(64, 3, activation='relu'),
            layers.GlobalAveragePooling1D(),
            layers.Dense(embedding_dim)
        ])
    
    def call(self, support_set, query_set):
        # Support set ì¸ì½”ë”©
        support_embeddings = self.encoder(support_set)
        
        # í´ë˜ìŠ¤ë³„ í”„ë¡œí† íƒ€ì… ê³„ì‚° (í‰ê· )
        prototypes = tf.reduce_mean(support_embeddings, axis=1)
        
        # Query set ì¸ì½”ë”©
        query_embeddings = self.encoder(query_set)
        
        # ìœ í´ë¦¬ë“œ ê±°ë¦¬ ê³„ì‚°
        distances = euclidean_distance(query_embeddings, prototypes)
        
        # ì†Œí”„íŠ¸ë§¥ìŠ¤ë¡œ í™•ë¥  ë³€í™˜
        return tf.nn.softmax(-distances)

def euclidean_distance(x, y):
    """ìœ í´ë¦¬ë“œ ê±°ë¦¬ ê³„ì‚°"""
    n = tf.shape(x)[0]
    m = tf.shape(y)[0]
    
    x = tf.tile(tf.expand_dims(x, 1), [1, m, 1])
    y = tf.tile(tf.expand_dims(y, 0), [n, 1, 1])
    
    return tf.reduce_sum(tf.square(x - y), axis=2)

# Few-Shot Learning í•™ìŠµ
model = PrototypicalNetwork()

# 5-way 1-shot (5ê°œ í´ë˜ìŠ¤, ê° 1ê°œ ìƒ˜í”Œ)
support_set = get_support_set(n_classes=5, n_samples=1)
query_set = get_query_set(n_samples=15)

predictions = model(support_set, query_set)
```

---

## ğŸ“ˆ Phase 2: ì ì§„ì  í•™ìŠµ (Online/Incremental Learning)

### 2.1 Incremental Learning (ì¦ë¶„ í•™ìŠµ) â­â­â­â­â­

**ê°œë…**: ìƒˆë¡œìš´ ë°ì´í„°ê°€ ë“¤ì–´ì˜¬ ë•Œë§ˆë‹¤ ëª¨ë¸ ì—…ë°ì´íŠ¸

```python
import numpy as np
from sklearn.linear_model import SGDClassifier
from river import stream, tree

class IncrementalFirePredictor:
    def __init__(self):
        # SGD ë¶„ë¥˜ê¸° (ì˜¨ë¼ì¸ í•™ìŠµ ê°€ëŠ¥)
        self.model = SGDClassifier(
            loss='log_loss',
            learning_rate='optimal',
            warm_start=True  # ì´ì „ í•™ìŠµ ìœ ì§€
        )
        
        self.is_fitted = False
        self.sample_count = 0
        self.performance_history = []
    
    def partial_fit(self, X_new, y_new):
        """ìƒˆë¡œìš´ ë°ì´í„°ë¡œ ì ì§„ì  í•™ìŠµ"""
        if not self.is_fitted:
            # ì²« í•™ìŠµ
            self.model.partial_fit(X_new, y_new, classes=np.array([0, 1, 2]))
            self.is_fitted = True
        else:
            # ì¦ë¶„ í•™ìŠµ
            self.model.partial_fit(X_new, y_new)
        
        self.sample_count += len(X_new)
        
        # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
        score = self.model.score(X_new, y_new)
        self.performance_history.append({
            'sample_count': self.sample_count,
            'accuracy': score,
            'timestamp': datetime.now()
        })
    
    def predict(self, X):
        return self.model.predict(X)
    
    def should_retrain(self, threshold=0.05):
        """ì¬í•™ìŠµ í•„ìš” ì—¬ë¶€ íŒë‹¨"""
        if len(self.performance_history) < 10:
            return False
        
        # ìµœê·¼ 10ê°œ ë°°ì¹˜ì˜ í‰ê·  ì„±ëŠ¥
        recent_avg = np.mean([h['accuracy'] for h in self.performance_history[-10:]])
        
        # ì´ˆê¸° 10ê°œ ë°°ì¹˜ì˜ í‰ê·  ì„±ëŠ¥
        initial_avg = np.mean([h['accuracy'] for h in self.performance_history[:10]])
        
        # ì„±ëŠ¥ ì €í•˜ í™•ì¸
        if initial_avg - recent_avg > threshold:
            return True
        
        return False

# ì‚¬ìš© ì˜ˆì‹œ
predictor = IncrementalFirePredictor()

# ì´ˆê¸° í•™ìŠµ
predictor.partial_fit(X_initial, y_initial)

# ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„°ë¡œ ì§€ì†ì  í•™ìŠµ
for X_batch, y_batch in data_stream():
    # ì˜ˆì¸¡
    predictions = predictor.predict(X_batch)
    
    # ì‹¤ì œ ê²°ê³¼ í™•ì¸ í›„ í•™ìŠµ
    predictor.partial_fit(X_batch, y_batch)
    
    # ì¬í•™ìŠµ í•„ìš” ì—¬ë¶€ í™•ì¸
    if predictor.should_retrain():
        print("ì„±ëŠ¥ ì €í•˜ ê°ì§€ - ì „ì²´ ì¬í•™ìŠµ í•„ìš”")
```

#### River ë¼ì´ë¸ŒëŸ¬ë¦¬ í™œìš© (ì˜¨ë¼ì¸ í•™ìŠµ)
```python
from river import tree, metrics, stream

# ì˜¨ë¼ì¸ ê²°ì • íŠ¸ë¦¬
model = tree.HoeffdingTreeClassifier()

# ë©”íŠ¸ë¦­
metric = metrics.Accuracy()

# ë°ì´í„° ìŠ¤íŠ¸ë¦¼
for x, y in stream.iter_csv('fire_sensor_stream.csv'):
    # ì˜ˆì¸¡
    y_pred = model.predict_one(x)
    
    # ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
    metric.update(y, y_pred)
    
    # ëª¨ë¸ í•™ìŠµ
    model.learn_one(x, y)
    
    print(f"Accuracy: {metric.get():.4f}")
```

---

### 2.2 Active Learning (ëŠ¥ë™ í•™ìŠµ) â­â­â­â­â­

**ê°œë…**: ëª¨ë¸ì´ ê°€ì¥ ë¶ˆí™•ì‹¤í•œ ìƒ˜í”Œì— ëŒ€í•´ ë ˆì´ë¸” ìš”ì²­

```python
from modAL.models import ActiveLearner
from modAL.uncertainty import uncertainty_sampling
from sklearn.ensemble import RandomForestClassifier

class ActiveFirePredictor:
    def __init__(self, X_initial, y_initial):
        self.learner = ActiveLearner(
            estimator=RandomForestClassifier(n_estimators=100),
            query_strategy=uncertainty_sampling,
            X_training=X_initial,
            y_training=y_initial
        )
        
        self.unlabeled_pool = []
        self.query_count = 0
    
    def add_to_unlabeled_pool(self, X_unlabeled):
        """ë ˆì´ë¸” ì—†ëŠ” ë°ì´í„° ì¶”ê°€"""
        self.unlabeled_pool.extend(X_unlabeled)
    
    def query_next_samples(self, n_instances=10):
        """ë¶ˆí™•ì‹¤ì„±ì´ ë†’ì€ ìƒ˜í”Œ ì„ íƒ"""
        if len(self.unlabeled_pool) == 0:
            return None
        
        X_pool = np.array(self.unlabeled_pool)
        
        # ë¶ˆí™•ì‹¤ì„± ê¸°ë°˜ ìƒ˜í”Œ ì„ íƒ
        query_idx, query_instances = self.learner.query(X_pool, n_instances=n_instances)
        
        self.query_count += n_instances
        
        return query_idx, query_instances
    
    def teach(self, X_new, y_new):
        """ì„ íƒëœ ìƒ˜í”Œë¡œ í•™ìŠµ"""
        self.learner.teach(X_new, y_new)
    
    def predict_proba(self, X):
        """í™•ë¥  ì˜ˆì¸¡"""
        return self.learner.predict_proba(X)
    
    def get_uncertainty_score(self, X):
        """ë¶ˆí™•ì‹¤ì„± ì ìˆ˜ ê³„ì‚°"""
        proba = self.predict_proba(X)
        # ì—”íŠ¸ë¡œí”¼ ê³„ì‚°
        entropy = -np.sum(proba * np.log(proba + 1e-10), axis=1)
        return entropy

# ì‚¬ìš© ì˜ˆì‹œ
active_predictor = ActiveFirePredictor(X_initial, y_initial)

# ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„° ì²˜ë¦¬
for X_batch in streaming_data():
    # ì˜ˆì¸¡
    predictions = active_predictor.learner.predict(X_batch)
    uncertainty = active_predictor.get_uncertainty_score(X_batch)
    
    # ë¶ˆí™•ì‹¤ì„±ì´ ë†’ì€ ìƒ˜í”Œë§Œ ë ˆì´ë¸”ë§ ìš”ì²­
    high_uncertainty_idx = np.where(uncertainty > 1.0)[0]
    
    if len(high_uncertainty_idx) > 0:
        X_uncertain = X_batch[high_uncertainty_idx]
        
        # ì „ë¬¸ê°€ì—ê²Œ ë ˆì´ë¸”ë§ ìš”ì²­ (ì›¹ ì¸í„°í˜ì´ìŠ¤ ë“±)
        y_labeled = request_labels_from_expert(X_uncertain)
        
        # í•™ìŠµ
        active_predictor.teach(X_uncertain, y_labeled)
        
        print(f"Queried {len(high_uncertainty_idx)} samples for labeling")
```

#### ì¿¼ë¦¬ ì „ëµ ë¹„êµ
```python
from modAL.uncertainty import (
    uncertainty_sampling,    # ë¶ˆí™•ì‹¤ì„± ìƒ˜í”Œë§
    margin_sampling,         # ë§ˆì§„ ìƒ˜í”Œë§
    entropy_sampling         # ì—”íŠ¸ë¡œí”¼ ìƒ˜í”Œë§
)

def compare_query_strategies(X_train, y_train, X_pool, X_test, y_test):
    """ë‹¤ì–‘í•œ ì¿¼ë¦¬ ì „ëµ ë¹„êµ"""
    strategies = {
        'Uncertainty': uncertainty_sampling,
        'Margin': margin_sampling,
        'Entropy': entropy_sampling
    }
    
    results = {}
    
    for name, strategy in strategies.items():
        learner = ActiveLearner(
            estimator=RandomForestClassifier(n_estimators=50),
            query_strategy=strategy,
            X_training=X_train,
            y_training=y_train
        )
        
        # 10ë²ˆ ë°˜ë³µ ì¿¼ë¦¬
        for _ in range(10):
            query_idx, query_instances = learner.query(X_pool, n_instances=10)
            
            # ì‹¤ì œë¡œëŠ” ì „ë¬¸ê°€ê°€ ë ˆì´ë¸”ë§
            y_new = y_pool[query_idx]  # ì‹œë®¬ë ˆì´ì…˜
            
            learner.teach(query_instances, y_new)
            
            # í’€ì—ì„œ ì œê±°
            X_pool = np.delete(X_pool, query_idx, axis=0)
            y_pool = np.delete(y_pool, query_idx)
        
        # ìµœì¢… ì„±ëŠ¥
        score = learner.score(X_test, y_test)
        results[name] = score
    
    return results
```

---

### 2.3 Semi-Supervised Learning (ì¤€ì§€ë„ í•™ìŠµ) â­â­â­â­

**ê°œë…**: ì†ŒëŸ‰ì˜ ë ˆì´ë¸” ë°ì´í„° + ëŒ€ëŸ‰ì˜ ë¬´ë ˆì´ë¸” ë°ì´í„° í™œìš©

```python
from sklearn.semi_supervised import LabelPropagation, LabelSpreading
import numpy as np

class SemiSupervisedFirePredictor:
    def __init__(self, kernel='rbf', gamma=20):
        self.model = LabelPropagation(
            kernel=kernel,
            gamma=gamma,
            max_iter=1000
        )
    
    def fit(self, X_labeled, y_labeled, X_unlabeled):
        """ë ˆì´ë¸” + ë¬´ë ˆì´ë¸” ë°ì´í„°ë¡œ í•™ìŠµ"""
        # ë¬´ë ˆì´ë¸” ë°ì´í„°ëŠ” -1ë¡œ í‘œì‹œ
        X_combined = np.vstack([X_labeled, X_unlabeled])
        y_combined = np.hstack([y_labeled, [-1] * len(X_unlabeled)])
        
        self.model.fit(X_combined, y_combined)
        
        # ë¬´ë ˆì´ë¸” ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡ëœ ë ˆì´ë¸”
        predicted_labels = self.model.transduction_[len(y_labeled):]
        
        return predicted_labels
    
    def predict(self, X):
        return self.model.predict(X)

# Pseudo-Labeling (ìê°€ í•™ìŠµ)
class PseudoLabeling:
    def __init__(self, base_model, confidence_threshold=0.9):
        self.model = base_model
        self.threshold = confidence_threshold
    
    def fit(self, X_labeled, y_labeled, X_unlabeled, n_iterations=10):
        """Pseudo-Labelingìœ¼ë¡œ í•™ìŠµ"""
        # ì´ˆê¸° í•™ìŠµ
        self.model.fit(X_labeled, y_labeled)
        
        for iteration in range(n_iterations):
            # ë¬´ë ˆì´ë¸” ë°ì´í„° ì˜ˆì¸¡
            proba = self.model.predict_proba(X_unlabeled)
            max_proba = np.max(proba, axis=1)
            
            # ë†’ì€ ì‹ ë¢°ë„ ìƒ˜í”Œ ì„ íƒ
            confident_idx = np.where(max_proba >= self.threshold)[0]
            
            if len(confident_idx) == 0:
                print(f"Iteration {iteration}: No confident predictions")
                break
            
            # Pseudo ë ˆì´ë¸” ìƒì„±
            pseudo_labels = np.argmax(proba[confident_idx], axis=1)
            X_pseudo = X_unlabeled[confident_idx]
            
            # ë ˆì´ë¸” ë°ì´í„°ì— ì¶”ê°€
            X_labeled = np.vstack([X_labeled, X_pseudo])
            y_labeled = np.hstack([y_labeled, pseudo_labels])
            
            # ë¬´ë ˆì´ë¸” í’€ì—ì„œ ì œê±°
            X_unlabeled = np.delete(X_unlabeled, confident_idx, axis=0)
            
            # ì¬í•™ìŠµ
            self.model.fit(X_labeled, y_labeled)
            
            print(f"Iteration {iteration}: Added {len(confident_idx)} pseudo-labeled samples")
        
        return X_labeled, y_labeled

# ì‚¬ìš© ì˜ˆì‹œ
from sklearn.ensemble import RandomForestClassifier

# ì´ˆê¸° ì†ŒëŸ‰ì˜ ë ˆì´ë¸” ë°ì´í„°
X_labeled_initial = X_train[:100]
y_labeled_initial = y_train[:100]

# ëŒ€ëŸ‰ì˜ ë¬´ë ˆì´ë¸” ë°ì´í„°
X_unlabeled = X_train[100:]

# Pseudo-Labeling
pseudo_learner = PseudoLabeling(
    base_model=RandomForestClassifier(n_estimators=100),
    confidence_threshold=0.95
)

X_labeled_final, y_labeled_final = pseudo_learner.fit(
    X_labeled_initial, 
    y_labeled_initial, 
    X_unlabeled,
    n_iterations=10
)

print(f"Final labeled dataset size: {len(X_labeled_final)}")
```

---

## ğŸ”„ Phase 3: Self-Upgrade (ìê°€ ê°œì„ )

### 3.1 Model Versioning & A/B Testing â­â­â­â­â­

**ê°œë…**: ì—¬ëŸ¬ ëª¨ë¸ ë²„ì „ì„ ìœ ì§€í•˜ê³  ì„±ëŠ¥ ë¹„êµ

```python
import mlflow
from datetime import datetime
import joblib

class ModelVersionManager:
    def __init__(self, model_name="fire_predictor"):
        self.model_name = model_name
        self.versions = []
        self.current_version = None
        self.performance_log = []
        
        mlflow.set_experiment(model_name)
    
    def register_model(self, model, metadata):
        """ìƒˆ ëª¨ë¸ ë²„ì „ ë“±ë¡"""
        version_id = f"v{len(self.versions) + 1}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        with mlflow.start_run(run_name=version_id):
            # ëª¨ë¸ ì €ì¥
            mlflow.sklearn.log_model(model, "model")
            
            # ë©”íƒ€ë°ì´í„° ë¡œê¹…
            mlflow.log_params(metadata)
            
            # ëª¨ë¸ íŒŒì¼ ì €ì¥
            model_path = f"models/{self.model_name}/{version_id}.pkl"
            joblib.dump(model, model_path)
            
            self.versions.append({
                'version_id': version_id,
                'model': model,
                'metadata': metadata,
                'created_at': datetime.now(),
                'status': 'candidate'  # candidate, champion, archived
            })
        
        return version_id
    
    def ab_test(self, X_test, y_test, n_samples=1000):
        """A/B í…ŒìŠ¤íŠ¸ ìˆ˜í–‰"""
        if len(self.versions) < 2:
            print("Need at least 2 models for A/B testing")
            return
        
        results = []
        
        for version in self.versions[-2:]:  # ìµœê·¼ 2ê°œ ë²„ì „
            model = version['model']
            version_id = version['version_id']
            
            # ì˜ˆì¸¡
            predictions = model.predict(X_test[:n_samples])
            
            # ì„±ëŠ¥ í‰ê°€
            from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
            
            metrics = {
                'version_id': version_id,
                'accuracy': accuracy_score(y_test[:n_samples], predictions),
                'precision': precision_score(y_test[:n_samples], predictions, average='weighted'),
                'recall': recall_score(y_test[:n_samples], predictions, average='weighted'),
                'f1': f1_score(y_test[:n_samples], predictions, average='weighted'),
                'test_samples': n_samples
            }
            
            results.append(metrics)
        
        # ì±”í”¼ì–¸ ì„ ì •
        best_version = max(results, key=lambda x: x['f1'])
        self.promote_to_champion(best_version['version_id'])
        
        return results
    
    def promote_to_champion(self, version_id):
        """ì±”í”¼ì–¸ ëª¨ë¸ë¡œ ìŠ¹ê²©"""
        for version in self.versions:
            if version['version_id'] == version_id:
                version['status'] = 'champion'
                self.current_version = version
            elif version['status'] == 'champion':
                version['status'] = 'archived'
        
        print(f"âœ… Model {version_id} promoted to champion")
    
    def get_champion_model(self):
        """í˜„ì¬ ì±”í”¼ì–¸ ëª¨ë¸ ë°˜í™˜"""
        return self.current_version['model'] if self.current_version else None

# ì‚¬ìš© ì˜ˆì‹œ
manager = ModelVersionManager("fire_risk_predictor")

# ëª¨ë¸ 1: Random Forest
rf_model = RandomForestClassifier(n_estimators=100)
rf_model.fit(X_train, y_train)
manager.register_model(rf_model, {'algorithm': 'RandomForest', 'n_estimators': 100})

# ëª¨ë¸ 2: XGBoost
xgb_model = XGBClassifier(n_estimators=100)
xgb_model.fit(X_train, y_train)
manager.register_model(xgb_model, {'algorithm': 'XGBoost', 'n_estimators': 100})

# A/B í…ŒìŠ¤íŠ¸
ab_results = manager.ab_test(X_test, y_test)

# ì±”í”¼ì–¸ ëª¨ë¸ ì‚¬ìš©
champion = manager.get_champion_model()
predictions = champion.predict(X_new)
```

---

### 3.2 Feedback Loop (í”¼ë“œë°± ë£¨í”„) â­â­â­â­â­

**ê°œë…**: ì˜ˆì¸¡ ê²°ê³¼ì— ëŒ€í•œ ì‹¤ì œ í”¼ë“œë°±ì„ ìˆ˜ì§‘í•˜ì—¬ ì¬í•™ìŠµ

```python
from datetime import datetime, timedelta
import pandas as pd

class FeedbackLoop:
    def __init__(self, model, feedback_window=7):
        self.model = model
        self.feedback_window = feedback_window  # days
        self.prediction_log = []
        self.feedback_log = []
    
    def predict_with_tracking(self, X, metadata=None):
        """ì˜ˆì¸¡ + ë¡œê¹…"""
        predictions = self.model.predict(X)
        probabilities = self.model.predict_proba(X)
        
        # ì˜ˆì¸¡ ë¡œê·¸ ì €ì¥
        for i, (pred, proba) in enumerate(zip(predictions, probabilities)):
            log_entry = {
                'timestamp': datetime.now(),
                'prediction': pred,
                'confidence': np.max(proba),
                'features': X[i].tolist(),
                'metadata': metadata,
                'feedback_received': False
            }
            self.prediction_log.append(log_entry)
        
        return predictions
    
    def add_feedback(self, prediction_id, actual_label, feedback_type='explicit'):
        """ì‹¤ì œ ê²°ê³¼ í”¼ë“œë°± ì¶”ê°€"""
        if prediction_id < len(self.prediction_log):
            self.prediction_log[prediction_id]['feedback_received'] = True
            self.prediction_log[prediction_id]['actual_label'] = actual_label
            self.prediction_log[prediction_id]['feedback_type'] = feedback_type
            
            self.feedback_log.append({
                'prediction_id': prediction_id,
                'predicted': self.prediction_log[prediction_id]['prediction'],
                'actual': actual_label,
                'timestamp': datetime.now(),
                'feedback_type': feedback_type
            })
    
    def calculate_online_metrics(self):
        """ì˜¨ë¼ì¸ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°"""
        if len(self.feedback_log) == 0:
            return None
        
        df = pd.DataFrame(self.feedback_log)
        
        # ìµœê·¼ í”¼ë“œë°±ë§Œ ì‚¬ìš©
        cutoff_time = datetime.now() - timedelta(days=self.feedback_window)
        df_recent = df[df['timestamp'] > cutoff_time]
        
        if len(df_recent) == 0:
            return None
        
        # ì •í™•ë„ ê³„ì‚°
        accuracy = (df_recent['predicted'] == df_recent['actual']).mean()
        
        # í´ë˜ìŠ¤ë³„ ì„±ëŠ¥
        class_performance = {}
        for class_label in df_recent['actual'].unique():
            mask = df_recent['actual'] == class_label
            class_acc = (df_recent[mask]['predicted'] == df_recent[mask]['actual']).mean()
            class_performance[f'class_{class_label}'] = class_acc
        
        return {
            'overall_accuracy': accuracy,
            'sample_count': len(df_recent),
            'class_performance': class_performance,
            'time_window': self.feedback_window
        }
    
    def should_retrain(self, accuracy_threshold=0.85):
        """ì¬í•™ìŠµ í•„ìš” ì—¬ë¶€ íŒë‹¨"""
        metrics = self.calculate_online_metrics()
        
        if metrics is None:
            return False
        
        # ì •í™•ë„ê°€ ì„ê³„ê°’ ì´í•˜ë©´ ì¬í•™ìŠµ
        if metrics['overall_accuracy'] < accuracy_threshold:
            return True
        
        # íŠ¹ì • í´ë˜ìŠ¤ ì„±ëŠ¥ì´ ë§¤ìš° ë‚®ìœ¼ë©´ ì¬í•™ìŠµ
        for class_acc in metrics['class_performance'].values():
            if class_acc < 0.7:
                return True
        
        return False
    
    def get_retraining_data(self):
        """ì¬í•™ìŠµìš© ë°ì´í„° ìˆ˜ì§‘"""
        X_retrain = []
        y_retrain = []
        
        for log in self.prediction_log:
            if log['feedback_received']:
                X_retrain.append(log['features'])
                y_retrain.append(log['actual_label'])
        
        return np.array(X_retrain), np.array(y_retrain)

# ì‚¬ìš© ì˜ˆì‹œ
feedback_loop = FeedbackLoop(model, feedback_window=7)

# ì‹¤ì‹œê°„ ì˜ˆì¸¡
for i, sensor_data in enumerate(streaming_sensor_data()):
    # ì˜ˆì¸¡
    prediction = feedback_loop.predict_with_tracking(
        sensor_data.reshape(1, -1),
        metadata={'sensor_id': 'sensor_001', 'location': 'Building_A'}
    )
    
    print(f"Prediction {i}: {['Safe', 'Warning', 'Danger'][prediction[0]]}")
    
    # ì‹¤ì œ ê²°ê³¼ í™•ì¸ (ë¹„ë™ê¸°)
    # ì˜ˆ: 5ë¶„ í›„ ì‹¤ì œ í™”ì¬ ë°œìƒ ì—¬ë¶€ í™•ì¸
    
# ë‚˜ì¤‘ì— í”¼ë“œë°± ìˆ˜ì‹ 
# ì˜ˆ: í™”ì¬ê°€ ì‹¤ì œë¡œ ë°œìƒí–ˆìŒì„ í™•ì¸
feedback_loop.add_feedback(prediction_id=123, actual_label=2, feedback_type='explicit')

# ì •ê¸°ì ìœ¼ë¡œ ì„±ëŠ¥ ì²´í¬
metrics = feedback_loop.calculate_online_metrics()
print(f"Online Accuracy: {metrics['overall_accuracy']:.2%}")

# ì¬í•™ìŠµ í•„ìš” ì—¬ë¶€ í™•ì¸
if feedback_loop.should_retrain():
    print("âš ï¸ Performance degradation detected - Retraining required")
    
    X_retrain, y_retrain = feedback_loop.get_retraining_data()
    model.fit(X_retrain, y_retrain)
```

---

### 3.3 AutoML & Hyperparameter Tuning â­â­â­â­

**ê°œë…**: ìë™ìœ¼ë¡œ ìµœì ì˜ ëª¨ë¸ê³¼ í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰

```python
from sklearn.model_selection import RandomizedSearchCV, GridSearchCV
from scipy.stats import randint, uniform
import optuna

class AutoFirePredictor:
    def __init__(self):
        self.best_model = None
        self.best_params = None
        self.search_history = []
    
    def hyperparameter_search(self, X_train, y_train, method='optuna'):
        """í•˜ì´í¼íŒŒë¼ë¯¸í„° ìë™ íƒìƒ‰"""
        if method == 'grid':
            return self._grid_search(X_train, y_train)
        elif method == 'random':
            return self._random_search(X_train, y_train)
        elif method == 'optuna':
            return self._optuna_search(X_train, y_train)
    
    def _optuna_search(self, X_train, y_train, n_trials=100):
        """Optunaë¥¼ ì´ìš©í•œ ë² ì´ì§€ì•ˆ ìµœì í™”"""
        def objective(trial):
            # ëª¨ë¸ ì„ íƒ
            model_name = trial.suggest_categorical('model', 
                ['RandomForest', 'XGBoost', 'LightGBM'])
            
            if model_name == 'RandomForest':
                params = {
                    'n_estimators': trial.suggest_int('n_estimators', 50, 300),
                    'max_depth': trial.suggest_int('max_depth', 3, 20),
                    'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),
                    'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10)
                }
                model = RandomForestClassifier(**params, random_state=42)
            
            elif model_name == 'XGBoost':
                params = {
                    'n_estimators': trial.suggest_int('n_estimators', 50, 300),
                    'max_depth': trial.suggest_int('max_depth', 3, 15),
                    'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
                    'subsample': trial.suggest_float('subsample', 0.6, 1.0),
                    'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0)
                }
                model = XGBClassifier(**params, random_state=42)
            
            elif model_name == 'LightGBM':
                params = {
                    'n_estimators': trial.suggest_int('n_estimators', 50, 300),
                    'max_depth': trial.suggest_int('max_depth', 3, 15),
                    'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
                    'num_leaves': trial.suggest_int('num_leaves', 20, 150)
                }
                model = LGBMClassifier(**params, random_state=42)
            
            # Cross-validation
            from sklearn.model_selection import cross_val_score
            scores = cross_val_score(model, X_train, y_train, cv=5, 
                                    scoring='f1_weighted')
            
            return scores.mean()
        
        # Optuna study
        study = optuna.create_study(direction='maximize')
        study.optimize(objective, n_trials=n_trials)
        
        # ìµœì  ëª¨ë¸ í•™ìŠµ
        best_params = study.best_params
        model_name = best_params.pop('model')
        
        if model_name == 'RandomForest':
            self.best_model = RandomForestClassifier(**best_params, random_state=42)
        elif model_name == 'XGBoost':
            self.best_model = XGBClassifier(**best_params, random_state=42)
        elif model_name == 'LightGBM':
            self.best_model = LGBMClassifier(**best_params, random_state=42)
        
        self.best_model.fit(X_train, y_train)
        self.best_params = best_params
        
        return study.best_value, best_params

# Neural Architecture Search (NAS)
class NeuralArchitectureSearch:
    def __init__(self, input_shape):
        self.input_shape = input_shape
        self.best_architecture = None
    
    def search_architecture(self, X_train, y_train, n_trials=50):
        """ì‹ ê²½ë§ êµ¬ì¡° ìë™ íƒìƒ‰"""
        def create_model(trial):
            # ë ˆì´ì–´ ìˆ˜
            n_layers = trial.suggest_int('n_layers', 1, 5)
            
            model = tf.keras.Sequential()
            model.add(layers.Input(shape=self.input_shape))
            
            for i in range(n_layers):
                # ìœ ë‹› ìˆ˜
                n_units = trial.suggest_int(f'n_units_l{i}', 32, 512)
                
                # í™œì„±í™” í•¨ìˆ˜
                activation = trial.suggest_categorical(
                    f'activation_l{i}', 
                    ['relu', 'tanh', 'elu']
                )
                
                # Dropout ë¹„ìœ¨
                dropout = trial.suggest_float(f'dropout_l{i}', 0.0, 0.5)
                
                model.add(layers.Dense(n_units, activation=activation))
                if dropout > 0:
                    model.add(layers.Dropout(dropout))
            
            # ì¶œë ¥ ë ˆì´ì–´
            model.add(layers.Dense(3, activation='softmax'))
            
            # ì˜µí‹°ë§ˆì´ì €
            learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)
            optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)
            
            model.compile(
                optimizer=optimizer,
                loss='sparse_categorical_crossentropy',
                metrics=['accuracy']
            )
            
            return model
        
        def objective(trial):
            model = create_model(trial)
            
            # ì¡°ê¸° ì¢…ë£Œ
            early_stopping = tf.keras.callbacks.EarlyStopping(
                monitor='val_loss',
                patience=10,
                restore_best_weights=True
            )
            
            # í•™ìŠµ
            history = model.fit(
                X_train, y_train,
                validation_split=0.2,
                epochs=100,
                batch_size=32,
                callbacks=[early_stopping],
                verbose=0
            )
            
            return history.history['val_accuracy'][-1]
        
        study = optuna.create_study(direction='maximize')
        study.optimize(objective, n_trials=n_trials)
        
        # ìµœì  êµ¬ì¡° ì €ì¥
        self.best_architecture = study.best_params
        
        return study.best_value, study.best_params

# ì‚¬ìš© ì˜ˆì‹œ
auto_predictor = AutoFirePredictor()

# ìë™ í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰
best_score, best_params = auto_predictor.hyperparameter_search(
    X_train, y_train, 
    method='optuna'
)

print(f"Best F1 Score: {best_score:.4f}")
print(f"Best Parameters: {best_params}")

# ìµœì  ëª¨ë¸ë¡œ ì˜ˆì¸¡
predictions = auto_predictor.best_model.predict(X_test)
```

---

### 3.4 Ensemble Learning (ì•™ìƒë¸”) â­â­â­â­â­

**ê°œë…**: ì—¬ëŸ¬ ëª¨ë¸ì˜ ì˜ˆì¸¡ì„ ê²°í•©í•˜ì—¬ ì„±ëŠ¥ í–¥ìƒ

```python
from sklearn.ensemble import VotingClassifier, StackingClassifier
from sklearn.linear_model import LogisticRegression

class EnsembleFirePredictor:
    def __init__(self):
        self.models = []
        self.ensemble = None
    
    def add_model(self, model, weight=1.0):
        """ì•™ìƒë¸”ì— ëª¨ë¸ ì¶”ê°€"""
        self.models.append((f'model_{len(self.models)}', model, weight))
    
    def build_voting_ensemble(self, voting='soft'):
        """íˆ¬í‘œ ê¸°ë°˜ ì•™ìƒë¸”"""
        estimators = [(name, model) for name, model, _ in self.models]
        weights = [weight for _, _, weight in self.models]
        
        self.ensemble = VotingClassifier(
            estimators=estimators,
            voting=voting,  # 'soft' or 'hard'
            weights=weights
        )
    
    def build_stacking_ensemble(self, meta_classifier=None):
        """ìŠ¤íƒœí‚¹ ì•™ìƒë¸”"""
        if meta_classifier is None:
            meta_classifier = LogisticRegression()
        
        estimators = [(name, model) for name, model, _ in self.models]
        
        self.ensemble = StackingClassifier(
            estimators=estimators,
            final_estimator=meta_classifier,
            cv=5
        )
    
    def fit(self, X_train, y_train):
        """ì•™ìƒë¸” í•™ìŠµ"""
        self.ensemble.fit(X_train, y_train)
    
    def predict(self, X):
        """ì•™ìƒë¸” ì˜ˆì¸¡"""
        return self.ensemble.predict(X)
    
    def predict_proba(self, X):
        """ì•™ìƒë¸” í™•ë¥  ì˜ˆì¸¡"""
        return self.ensemble.predict_proba(X)

# ì‚¬ìš© ì˜ˆì‹œ
ensemble = EnsembleFirePredictor()

# ë‹¤ì–‘í•œ ëª¨ë¸ ì¶”ê°€
ensemble.add_model(RandomForestClassifier(n_estimators=100), weight=1.0)
ensemble.add_model(XGBClassifier(n_estimators=100), weight=1.2)
ensemble.add_model(LGBMClassifier(n_estimators=100), weight=1.1)
ensemble.add_model(MLPClassifier(hidden_layers=(100, 50)), weight=0.8)

# íˆ¬í‘œ ì•™ìƒë¸”
ensemble.build_voting_ensemble(voting='soft')
ensemble.fit(X_train, y_train)

# ì˜ˆì¸¡
predictions = ensemble.predict(X_test)
probabilities = ensemble.predict_proba(X_test)

# ì„±ëŠ¥ í‰ê°€
from sklearn.metrics import classification_report
print(classification_report(y_test, predictions))
```

---

## ğŸ—ï¸ í†µí•© ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

```python
class SelfUpgradingFirePredictor:
    """ìê°€ ê°œì„  í™”ì¬ ì˜ˆì¸¡ ì‹œìŠ¤í…œ"""
    
    def __init__(self, initial_data=None):
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.version_manager = ModelVersionManager("fire_predictor")
        self.feedback_loop = None
        self.auto_tuner = AutoFirePredictor()
        self.ensemble = EnsembleFirePredictor()
        
        # ì´ˆê¸° ëª¨ë¸ í•™ìŠµ
        if initial_data is not None:
            self._bootstrap(initial_data)
    
    def _bootstrap(self, initial_data):
        """ì´ˆê¸° ë¶€íŠ¸ìŠ¤íŠ¸ë©"""
        X_initial, y_initial = initial_data
        
        # 1. ë°ì´í„° ì¦ê°•
        X_aug, y_aug = fire_risk_augmentation(X_initial, y_initial)
        
        # 2. ì´ˆê¸° ëª¨ë¸ í•™ìŠµ
        base_model = RandomForestClassifier(n_estimators=100)
        base_model.fit(X_aug, y_aug)
        
        # 3. ëª¨ë¸ ë“±ë¡
        self.version_manager.register_model(
            base_model,
            {'type': 'bootstrap', 'data_size': len(X_aug)}
        )
        
        # 4. í”¼ë“œë°± ë£¨í”„ ì´ˆê¸°í™”
        self.feedback_loop = FeedbackLoop(base_model)
    
    def predict(self, X, track=True):
        """ì˜ˆì¸¡ (í”¼ë“œë°± ì¶”ì  ì˜µì…˜)"""
        champion = self.version_manager.get_champion_model()
        
        if track and self.feedback_loop:
            return self.feedback_loop.predict_with_tracking(X)
        else:
            return champion.predict(X)
    
    def add_feedback(self, prediction_id, actual_label):
        """í”¼ë“œë°± ì¶”ê°€"""
        if self.feedback_loop:
            self.feedback_loop.add_feedback(prediction_id, actual_label)
    
    def check_and_upgrade(self, X_test, y_test):
        """ìë™ ì—…ê·¸ë ˆì´ë“œ ì²´í¬"""
        # 1. ì¬í•™ìŠµ í•„ìš” ì—¬ë¶€ í™•ì¸
        if self.feedback_loop and self.feedback_loop.should_retrain():
            print("ğŸ”„ Performance degradation detected - Starting upgrade process")
            
            # 2. ì¬í•™ìŠµ ë°ì´í„° ìˆ˜ì§‘
            X_retrain, y_retrain = self.feedback_loop.get_retraining_data()
            
            # 3. AutoMLë¡œ ìµœì  ëª¨ë¸ íƒìƒ‰
            print("ğŸ” Searching for optimal model...")
            best_score, best_params = self.auto_tuner.hyperparameter_search(
                X_retrain, y_retrain,
                method='optuna'
            )
            
            # 4. ìƒˆ ëª¨ë¸ ë“±ë¡
            new_model = self.auto_tuner.best_model
            version_id = self.version_manager.register_model(
                new_model,
                {'type': 'auto_upgrade', 'score': best_score, 'params': best_params}
            )
            
            # 5. A/B í…ŒìŠ¤íŠ¸
            print("ğŸ§ª Running A/B test...")
            ab_results = self.version_manager.ab_test(X_test, y_test)
            
            # 6. ì±”í”¼ì–¸ ëª¨ë¸ ì—…ë°ì´íŠ¸
            champion = self.version_manager.get_champion_model()
            self.feedback_loop.model = champion
            
            print(f"âœ… Upgrade complete - New champion: {version_id}")
            
            return True
        
        return False
    
    def get_status(self):
        """ì‹œìŠ¤í…œ ìƒíƒœ ë°˜í™˜"""
        metrics = self.feedback_loop.calculate_online_metrics() if self.feedback_loop else {}
        
        return {
            'champion_version': self.version_manager.current_version['version_id'] if self.version_manager.current_version else None,
            'total_versions': len(self.version_manager.versions),
            'online_metrics': metrics,
            'total_predictions': len(self.feedback_loop.prediction_log) if self.feedback_loop else 0,
            'feedback_count': len(self.feedback_loop.feedback_log) if self.feedback_loop else 0
        }

# ì‚¬ìš© ì˜ˆì‹œ
system = SelfUpgradingFirePredictor(initial_data=(X_train, y_train))

# ì‹¤ì‹œê°„ ì˜ˆì¸¡
for i, sensor_data in enumerate(streaming_data()):
    # ì˜ˆì¸¡ (ìë™ ì¶”ì )
    prediction = system.predict(sensor_data, track=True)
    print(f"Prediction {i}: {['Safe', 'Warning', 'Danger'][prediction[0]]}")
    
    # ë‚˜ì¤‘ì— ì‹¤ì œ ê²°ê³¼ í™•ì¸ í›„ í”¼ë“œë°±
    # system.add_feedback(i, actual_label)

# ì •ê¸°ì ìœ¼ë¡œ ì—…ê·¸ë ˆì´ë“œ ì²´í¬ (ì˜ˆ: ë§¤ì¼)
upgraded = system.check_and_upgrade(X_test, y_test)

# ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸
status = system.get_status()
print(f"System Status: {status}")
```

---

## ğŸ“‹ ì „ëµ ì„ íƒ ê°€ì´ë“œ

| ìƒí™© | ì¶”ì²œ ì „ëµ | ìš°ì„ ìˆœìœ„ |
|------|----------|---------|
| **ë°ì´í„°ê°€ ë§¤ìš° ì ìŒ (<100ê°œ)** | Transfer Learning + Few-Shot Learning | â­â­â­â­â­ |
| **ë ˆì´ë¸”ë§ ë¹„ìš©ì´ ë†’ìŒ** | Active Learning + Semi-Supervised | â­â­â­â­â­ |
| **ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„°** | Incremental Learning + Feedback Loop | â­â­â­â­â­ |
| **ë°ì´í„° ë¶ˆê· í˜•** | Data Augmentation + Synthetic Data | â­â­â­â­â­ |
| **ì§€ì†ì  ê°œì„  í•„ìš”** | AutoML + Model Versioning | â­â­â­â­ |
| **ë†’ì€ ì •í™•ë„ í•„ìš”** | Ensemble Learning | â­â­â­â­â­ |

---

## ğŸ¯ êµ¬í˜„ ë¡œë“œë§µ

### Phase 1: ì´ˆê¸° êµ¬ì¶• (1-2ì£¼)
1. âœ… Transfer Learningìœ¼ë¡œ ì´ˆê¸° ëª¨ë¸ êµ¬ì¶•
2. âœ… Data Augmentation íŒŒì´í”„ë¼ì¸ êµ¬ì¶•
3. âœ… ê¸°ë³¸ ì˜ˆì¸¡ API êµ¬í˜„

### Phase 2: ì ì§„ì  í•™ìŠµ (2-3ì£¼)
4. âœ… Incremental Learning êµ¬í˜„
5. âœ… Active Learning ì‹œìŠ¤í…œ êµ¬ì¶•
6. âœ… Feedback Loop êµ¬í˜„

### Phase 3: Self-Upgrade (3-4ì£¼)
7. âœ… Model Versioning ì‹œìŠ¤í…œ
8. âœ… A/B Testing í”„ë ˆì„ì›Œí¬
9. âœ… AutoML í†µí•©

### Phase 4: ìµœì í™” (ì§€ì†ì )
10. âœ… ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ
11. âœ… Ensemble ì „ëµ ê°œì„ 
12. âœ… ìë™í™” ë° ìŠ¤ì¼€ì¼ë§

---

ì´ ì „ëµë“¤ì„ ì¡°í•©í•˜ë©´ ë°ì´í„°ê°€ ë¶€ì¡±í•œ ìƒí™©ì—ì„œë„ íš¨ê³¼ì ì¸ í™”ì¬ ì˜ˆì¸¡ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•˜ê³ , ì‹œê°„ì´ ì§€ë‚¨ì— ë”°ë¼ ìë™ìœ¼ë¡œ ê°œì„ ë˜ëŠ” ì‹œìŠ¤í…œì„ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤! ğŸš€
